import streamlit as st

# ---------------------------
# UI ê¸°ë³¸ í‹€ (ëŒ€ì‹œë³´ë“œ í—¤ë” + ë©”ë‰´)
# ---------------------------

# í™”ë©´ ìƒë‹¨ í° ì œëª©
st.markdown("<h1 style='text-align:center;'>ğŸ“Š YouTube Analytics Dashboard</h1>", unsafe_allow_html=True)

# ì¢Œì¸¡ ë©”ë‰´
menu = st.sidebar.radio(
    "ğŸ“ ë©”ë‰´ ì„ íƒ",
    ["Dashboard í™ˆ", "ì±„ë„ ë¶„ì„", "ì˜ìƒ ë¶„ì„", "SEO ë¶„ì„", "ê²½ìŸ ì±„ë„"]
)

import re
from collections import Counter
from datetime import datetime, timezone
from typing import List, Dict, Tuple

import pandas as pd
import streamlit as st
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError


# ----------------------------
# ê¸°ë³¸ ì„¤ì •
# ----------------------------

st.set_page_config(
    page_title="YouTube íŠ¸ë Œë“œÂ·ì±„ë„ ë¶„ì„ê¸° (v3.0)",
    page_icon="ğŸ“Š",
    layout="wide",
)

st.markdown(
    """
    <style>
    /* ì „ì²´ í°íŠ¸/ì—¬ë°± ì¡°ê¸ˆ ë‹¤ë“¬ê¸° */
    .main-block {padding-top: 0rem;}
    .block-container {padding-top: 1.5rem;}
    </style>
    """,
    unsafe_allow_html=True,
)


# ----------------------------
# ìœ í‹¸ í•¨ìˆ˜
# ----------------------------

def get_api_key() -> str:
    """Streamlit Secrets ì—ì„œ API KEY ê°€ì ¸ì˜¤ê¸°"""
    key = st.secrets.get("YOUTUBE_API_KEY", "")
    if not key:
        st.error("âŒ YOUTUBE_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit â†’ App Settings â†’ Secrets ì—ì„œ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
    return key


def build_youtube(api_key: str):
    """YouTube í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return build("youtube", "v3", developerKey=api_key)


def parse_iso_duration(duration: str) -> int:
    """
    ISO8601 duration(ì˜ˆ: 'PT15M33S') â†’ ì´ˆ ë‹¨ìœ„ ì •ìˆ˜ë¡œ ë³€í™˜
    """
    if not duration:
        return 0
    pattern = re.compile(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?")
    match = pattern.match(duration)
    if not match:
        return 0
    hours, mins, secs = match.groups()
    hours = int(hours) if hours else 0
    mins = int(mins) if mins else 0
    secs = int(secs) if secs else 0
    return hours * 3600 + mins * 60 + secs


def weekday_kr_from_ts(ts: pd.Timestamp) -> str:
    """ìš”ì¼ì„ í•œêµ­ì–´ í•œ ê¸€ìë¡œ ë°˜í™˜"""
    mapping = {0: "ì›”", 1: "í™”", 2: "ìˆ˜", 3: "ëª©", 4: "ê¸ˆ", 5: "í† ", 6: "ì¼"}
    return mapping.get(ts.weekday(), "")


def extract_channel_id(raw: str) -> str:
    """
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê°’ì—ì„œ channelId ì¶”ì¶œ
    - UC ë¡œ ì‹œì‘í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - https://www.youtube.com/channel/UCxxxx í˜•ì‹ ì§€ì›
    ê·¸ ì™¸ ë³µì¡í•œ ê²½ìš°ëŠ” ì§€ì›í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜
    """
    raw = raw.strip()
    if "youtube.com/channel/" in raw:
        return raw.split("youtube.com/channel/")[-1].split("/")[0].split("?")[0]
    if "youtube.com/" in raw:
        # ê¸°íƒ€ URL ì˜ ë§ˆì§€ë§‰ path ë¥¼ ID ë¡œ ê°„ì£¼
        path = raw.split("youtube.com/")[-1]
        return path.split("/")[-1].split("?")[0]
    return raw


def safe_int(x):
    try:
        return int(x)
    except Exception:
        return 0


# ----------------------------
# ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ì ìš©)
# ----------------------------

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_videos_by_keyword(api_key: str, keyword: str, max_results: int) -> pd.DataFrame:
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ìƒ ëª©ë¡ ì¡°íšŒ (ìµœëŒ€ 30ê°œ ì •ë„ ê¶Œì¥)
    search.list â†’ videos.list 1íšŒë§Œ ì‚¬ìš©í•´ì„œ ì¿¼í„° ì ˆì•½
    """
    youtube = build_youtube(api_key)

    # search.list (max 50)
    max_results = max(1, min(max_results, 50))
    search_resp = youtube.search().list(
        part="snippet",
        q=keyword,
        type="video",
        order="relevance",
        maxResults=max_results,
    ).execute()

    video_ids = [item["id"]["videoId"] for item in search_resp.get("items", [])]
    if not video_ids:
        return pd.DataFrame()

    # videos.list
    videos_resp = youtube.videos().list(
        part="snippet,contentDetails,statistics",
        id=",".join(video_ids),
        maxResults=len(video_ids),
    ).execute()

    rows = []
    for item in videos_resp.get("items", []):
        snippet = item.get("snippet", {})
        stats = item.get("statistics", {})
        content = item.get("contentDetails", {})

        published_at = snippet.get("publishedAt")
        try:
            ts = pd.to_datetime(published_at)
        except Exception:
            ts = pd.NaT

        duration_sec = parse_iso_duration(content.get("duration", ""))

        rows.append(
            {
                "video_id": item.get("id"),
                "title": snippet.get("title"),
                "description": snippet.get("description", ""),
                "channel_title": snippet.get("channelTitle"),
                "channel_id": snippet.get("channelId"),
                "published_at": ts,
                "views": safe_int(stats.get("viewCount")),
                "likes": safe_int(stats.get("likeCount")),
                "comments": safe_int(stats.get("commentCount")),
                "duration_sec": duration_sec,
                "thumbnail_url": snippet.get("thumbnails", {})
                .get("medium", {})
                .get("url", ""),
            }
        )

    df = pd.DataFrame(rows)
    if df.empty:
        return df

    now = datetime.now(timezone.utc)
    df["days_since_publish"] = (now - df["published_at"]).dt.total_seconds() / (3600 * 24)
    df["days_since_publish"] = df["days_since_publish"].replace(0, 0.1)
    df["views_per_day"] = df["views"] / df["days_since_publish"]
    df["duration_min"] = df["duration_sec"] / 60
    df["weekday"] = df["published_at"].apply(weekday_kr_from_ts)
    df["publish_hour"] = df["published_at"].dt.hour

    # ì´ë¡ ìƒ ìµœëŒ€ ì‹œì²­ì‹œê°„(ë¶„) = ì˜ìƒ ê¸¸ì´(ë¶„) * ì¡°íšŒìˆ˜
    df["max_watch_time_min"] = df["duration_min"] * df["views"]

    return df.sort_values("views", ascending=False).reset_index(drop=True)


@st.cache_data(ttl=3600, show_spinner=False)
def fetch_channel_basic(api_key: str, channel_id: str) -> Dict:
    """ì±„ë„ ê¸°ë³¸ ì •ë³´ + í†µê³„"""
    youtube = build_youtube(api_key)
    resp = youtube.channels().list(
        part="snippet,statistics,contentDetails",
        id=channel_id,
        maxResults=1,
    ).execute()

    items = resp.get("items", [])
    if not items:
        return {}

    item = items[0]
    stats = item.get("statistics", {})
    snippet = item.get("snippet", {})

    return {
        "channel_id": item.get("id"),
        "title": snippet.get("title"),
        "description": snippet.get("description", ""),
        "published_at": pd.to_datetime(snippet.get("publishedAt")),
        "subscriber_count": safe_int(stats.get("subscriberCount")),
        "video_count": safe_int(stats.get("videoCount")),
        "view_count": safe_int(stats.get("viewCount")),
        "thumbnail_url": snippet.get("thumbnails", {}).get("medium", {}).get("url", ""),
    }


@st.cache_data(ttl=3600, show_spinner=False)
def fetch_channel_recent_videos(
    api_key: str, channel_id: str, max_results: int
) -> pd.DataFrame:
    """ì±„ë„ ìµœê·¼ ì—…ë¡œë“œ ì˜ìƒë“¤ (max_results â‰¦ 50 ê¶Œì¥)"""
    youtube = build_youtube(api_key)

    max_results = max(1, min(max_results, 50))
    search_resp = youtube.search().list(
        part="snippet",
        channelId=channel_id,
        type="video",
        order="date",
        maxResults=max_results,
    ).execute()

    video_ids = [item["id"]["videoId"] for item in search_resp.get("items", [])]
    if not video_ids:
        return pd.DataFrame()

    videos_resp = youtube.videos().list(
        part="snippet,contentDetails,statistics",
        id=",".join(video_ids),
        maxResults=len(video_ids),
    ).execute()

    rows = []
    for item in videos_resp.get("items", []):
        snippet = item.get("snippet", {})
        stats = item.get("statistics", {})
        content = item.get("contentDetails", {})

        published_at = snippet.get("publishedAt")
        try:
            ts = pd.to_datetime(published_at)
        except Exception:
            ts = pd.NaT

        duration_sec = parse_iso_duration(content.get("duration", ""))

        rows.append(
            {
                "video_id": item.get("id"),
                "title": snippet.get("title"),
                "description": snippet.get("description", ""),
                "published_at": ts,
                "views": safe_int(stats.get("viewCount")),
                "likes": safe_int(stats.get("likeCount")),
                "comments": safe_int(stats.get("commentCount")),
                "duration_sec": duration_sec,
                "thumbnail_url": snippet.get("thumbnails", {})
                .get("medium", {})
                .get("url", ""),
            }
        )

    df = pd.DataFrame(rows)
    if df.empty:
        return df

    now = datetime.now(timezone.utc)
    df["days_since_publish"] = (now - df["published_at"]).dt.total_seconds() / (3600 * 24)
    df["days_since_publish"] = df["days_since_publish"].replace(0, 0.1)
    df["views_per_day"] = df["views"] / df["days_since_publish"]
    df["duration_min"] = df["duration_sec"] / 60
    df["weekday"] = df["published_at"].apply(weekday_kr_from_ts)
    df["publish_hour"] = df["published_at"].dt.hour
    df["max_watch_time_min"] = df["duration_min"] * df["views"]

    return df.sort_values("published_at", ascending=False).reset_index(drop=True)


# ----------------------------
# SEO / í‚¤ì›Œë“œ ë¶„ì„
# ----------------------------

def extract_keywords_from_titles(titles: List[str], top_n: int = 30) -> pd.DataFrame:
    """
    ì œëª© ë¦¬ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ì•„ì£¼ ë‹¨ìˆœí•œ ë°©ì‹, ì°¸ê³ ìš©)
    """
    joined = " ".join(titles).lower()
    tokens = re.findall(r"[ê°€-í£a-zA-Z0-9]+", joined)

    stopwords = {
        "ì˜ìƒ",
        "official",
        "video",
        "the",
        "and",
        "for",
        "with",
        "full",
        "ver",
        "episode",
        "ep",
        "live",
        "tv",
        "show",
        "channel",
        "shorts",
    }

    filtered = [t for t in tokens if len(t) >= 2 and t not in stopwords]
    counts = Counter(filtered)

    if not counts:
        return pd.DataFrame(columns=["keyword", "count"])

    data = pd.DataFrame(
        [{"keyword": k, "count": v} for k, v in counts.most_common(top_n)]
    )
    return data


def render_keyword_suggestions(df: pd.DataFrame):
    st.subheader("ğŸ” SEO í‚¤ì›Œë“œ/íƒœê·¸ ì•„ì´ë””ì–´")

    if df.empty:
        st.info("ë¶„ì„í•  ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    kw_df = extract_keywords_from_titles(df["title"].tolist(), top_n=30)
    if kw_df.empty:
        st.info("ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. ì œëª© íŒ¨í„´ì´ ë„ˆë¬´ ë‹¨ìˆœí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    col1, col2 = st.columns([2, 1])

    with col1:
        st.markdown("**ì œëª©ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ TOP 30**")
        st.dataframe(kw_df, use_container_width=True, hide_index=True)

    with col2:
        st.markdown("**íƒœê·¸ë¡œ ì¨ë³¼ ë§Œí•œ í›„ë³´**")
        tag_candidates = kw_df.query("count >= 2")["keyword"].tolist()[:15]
        st.code(", ".join(tag_candidates), language="text")

        st.caption("â€» ë‹¨ìˆœ ë¹ˆë„ ê¸°ì¤€ì´ë¯€ë¡œ ì‹¤ì œ ê²€ìƒ‰ëŸ‰/ê²½ìŸë„ì™€ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


# ----------------------------
# ìš”ì•½ ë©”ì‹œì§€ ìƒì„± (ë£° ê¸°ë°˜)
# ----------------------------

def make_simple_summary_for_channel(df: pd.DataFrame) -> str:
    if df.empty:
        return "ìµœê·¼ ì˜ìƒ ë°ì´í„°ê°€ ì—†ì–´ íŒ¨í„´ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    n = len(df)
    avg_views = int(df["views"].mean())
    median_views = int(df["views"].median())
    max_views = int(df["views"].max())

    # ê¸¸ì´ ê´€ë ¨
    short = df[df["duration_min"] <= 8]
    long = df[df["duration_min"] >= 20]

    parts = []
    parts.append(f"ìµœê·¼ {n}ê°œ ì˜ìƒ ê¸°ì¤€ìœ¼ë¡œ í‰ê·  ì¡°íšŒìˆ˜ëŠ” ì•½ {avg_views:,}íšŒ, ì¤‘ì•™ê°’ì€ {median_views:,}íšŒì…ë‹ˆë‹¤.")
    parts.append(f"ê°€ì¥ ë§ì´ ë³¸ ì˜ìƒì€ ì•½ {max_views:,}íšŒê¹Œì§€ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.")

    if not short.empty and not long.empty:
        short_avg = int(short["views"].mean())
        long_avg = int(long["views"].mean())
        if short_avg > long_avg * 1.3:
            parts.append(
                f"8ë¶„ ì´í•˜ ì§§ì€ ì˜ìƒì˜ í‰ê·  ì¡°íšŒìˆ˜ê°€ {short_avg:,}íšŒë¡œ, 20ë¶„ ì´ìƒ ê¸´ ì˜ìƒ({long_avg:,}íšŒ)ë³´ë‹¤ ê½¤ ì˜ ë‚˜ì˜¤ëŠ” í¸ì…ë‹ˆë‹¤. "
                "ì§§ì€ ê¸¸ì´ì˜ ì½˜í…ì¸  ë¹„ì¤‘ì„ ì¡°ê¸ˆ ë” ëŠ˜ë ¤ë³´ëŠ” ê²ƒë„ ì¢‹ê² ìŠµë‹ˆë‹¤."
            )
        elif long_avg > short_avg * 1.3:
            parts.append(
                f"20ë¶„ ì´ìƒ ê¸´ ì˜ìƒì˜ í‰ê·  ì¡°íšŒìˆ˜ê°€ {long_avg:,}íšŒë¡œ, 8ë¶„ ì´í•˜ ì˜ìƒ({short_avg:,}íšŒ)ë³´ë‹¤ ìœ ë¦¬í•©ë‹ˆë‹¤. "
                "ê¹Šì´ ìˆëŠ” ì¥í¸ ì½˜í…ì¸ ê°€ ì±„ë„ì— ì˜ ë§ëŠ” í¸ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."
            )

    # ìš”ì¼ íŒ¨í„´
    weekday_mean = df.groupby("weekday")["views"].mean().sort_values(ascending=False)
    if len(weekday_mean) >= 3:
        best_day = weekday_mean.index[0]
        parts.append(
            f"ìš”ì¼ë³„ í‰ê·  ì¡°íšŒìˆ˜ëŠ” **{best_day}ìš”ì¼ ì—…ë¡œë“œë¶„**ì´ ê°€ì¥ ë†’ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. "
            "í•´ë‹¹ ìš”ì¼ ì „í›„ë¡œ ì¤‘ìš”í•œ ì˜ìƒì„ ë°°ì¹˜í•˜ëŠ” ì „ëµì„ ê³ ë ¤í•´ë³¼ ë§Œí•©ë‹ˆë‹¤."
        )

    return "\n\n".join(parts)


# ----------------------------
# í™”ë©´ êµ¬ì„± í•¨ìˆ˜ë“¤
# ----------------------------

def render_header():
    st.title("ğŸ“Š YouTube íŠ¸ë Œë“œÂ·ì±„ë„ ë¶„ì„ê¸° (v3.0)")
    st.caption("í‚¤ì›Œë“œ / ì±„ë„ ë‹¨ìœ„ë¡œ íŠ¸ë Œë“œ, íŒ¨í„´, SEO, ê²½ìŸê¹Œì§€ í•œ ë²ˆì— ë¶„ì„í•˜ëŠ” ëŒ€ì‹œë³´ë“œì…ë‹ˆë‹¤.")


def render_basic_stats_cards_for_videos(df: pd.DataFrame, title: str):
    st.subheader(title)

    if df.empty:
        st.info("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    total_views = int(df["views"].sum())
    avg_views = int(df["views"].mean())
    median_views = int(df["views"].median())
    total_max_watch_min = int(df["max_watch_time_min"].sum())

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ì´ ì¡°íšŒìˆ˜", f"{total_views:,}")
    col2.metric("ì˜ìƒ ìˆ˜", f"{len(df):,}")
    col3.metric("í‰ê·  ì¡°íšŒìˆ˜", f"{avg_views:,}")
    col4.metric("ì´ë¡ ìƒ ìµœëŒ€ ì‹œì²­ì‹œê°„(ë¶„)", f"{total_max_watch_min:,}")

    st.caption("â€» 'ì´ë¡ ìƒ ìµœëŒ€ ì‹œì²­ì‹œê°„'ì€ ì˜ìƒ ì „ì²´ë¥¼ ëê¹Œì§€ ë³¸ë‹¤ê³  ê°€ì •í–ˆì„ ë•Œì˜ ê°’ìœ¼ë¡œ, ì‹¤ì œ ì‹œì²­ì‹œê°„/ìœ ì§€ìœ¨ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def render_video_table(df: pd.DataFrame):
    if df.empty:
        return

    show_cols = [
        "title",
        "views",
        "views_per_day",
        "duration_min",
        "weekday",
        "publish_hour",
        "published_at",
    ]
    rename = {
        "title": "ì œëª©",
        "views": "ì¡°íšŒìˆ˜",
        "views_per_day": "ì¼ í‰ê·  ì¡°íšŒìˆ˜",
        "duration_min": "ê¸¸ì´(ë¶„)",
        "weekday": "ìš”ì¼",
        "publish_hour": "ì—…ë¡œë“œ ì‹œê°„(ì‹œ)",
        "published_at": "ì—…ë¡œë“œ ì¼ì‹œ",
    }

    st.markdown("#### ğŸ“‹ ìƒì„¸ ì˜ìƒ ë¦¬ìŠ¤íŠ¸")
    st.dataframe(
        df[show_cols].rename(columns=rename),
        use_container_width=True,
        hide_index=True,
    )


def render_pattern_charts(df: pd.DataFrame):
    if df.empty:
        return

    st.subheader("ğŸ“ˆ íŒ¨í„´ ë¶„ì„")

    c1, c2 = st.columns(2)

    with c1:
        st.markdown("**ìš”ì¼ë³„ í‰ê·  ì¡°íšŒìˆ˜**")
        weekday_order = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
        weekday_mean = (
            df.groupby("weekday")["views"]
            .mean()
            .reindex(weekday_order)
            .dropna()
            .astype(int)
        )
        if not weekday_mean.empty:
            st.bar_chart(weekday_mean)

    with c2:
        st.markdown("**ì—…ë¡œë“œ ì‹œê°„ëŒ€ë³„ í‰ê·  ì¡°íšŒìˆ˜**")
        hour_mean = df.groupby("publish_hour")["views"].mean().astype(int)
        if not hour_mean.empty:
            st.bar_chart(hour_mean)

    c3, c4 = st.columns(2)

    with c3:
        st.markdown("**ì˜ìƒ ê¸¸ì´(ë¶„) vs ì¡°íšŒìˆ˜**")
        st.scatter_chart(
            df[["duration_min", "views"]].rename(
                columns={"duration_min": "ê¸¸ì´(ë¶„)", "views": "ì¡°íšŒìˆ˜"}
            )
        )

    with c4:
        st.markdown("**ì—…ë¡œë“œ í›„ ê²½ê³¼ì¼ vs ì¼ í‰ê·  ì¡°íšŒìˆ˜**")
        st.scatter_chart(
            df[["days_since_publish", "views_per_day"]].rename(
                columns={
                    "days_since_publish": "ì—…ë¡œë“œ í›„ ê²½ê³¼ì¼",
                    "views_per_day": "ì¼ í‰ê·  ì¡°íšŒìˆ˜",
                }
            )
        )


def render_top_thumbnails(df: pd.DataFrame):
    if df.empty:
        return
    st.subheader("ğŸ† ìƒìœ„ ì„±ê³¼ ì˜ìƒ ì¸ë„¤ì¼ (TOP 3)")
    top3 = df.sort_values("views", ascending=False).head(3)
    cols = st.columns(3)
    for col, (_, row) in zip(cols, top3.iterrows()):
        with col:
            if row["thumbnail_url"]:
                st.image(row["thumbnail_url"])
            st.markdown(f"**{row['title']}**")
            st.caption(f"ì¡°íšŒìˆ˜: {row['views']:,}íšŒ")


# ----------------------------
# ê° ë¶„ì„ ëª¨ë“œ ë Œë”ë§
# ----------------------------

def page_keyword_trend(api_key: str, video_limit: int):
    render_header()
    st.markdown("### ğŸ¯ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„")

    keyword = st.text_input("ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì‹œë‹ˆì–´ ì‡¼í•‘, ê±´ê°•, ìš”ë¦¬ ë“±)")
    st.caption("â€» ê°€ì ¸ì˜¬ ì˜ìƒ ìˆ˜ë¥¼ ë„ˆë¬´ í¬ê²Œ ì„¤ì •í•˜ë©´ YouTube API í• ë‹¹ëŸ‰ì´ ë¹¨ë¦¬ ì†Œëª¨ë©ë‹ˆë‹¤. (5~15ê°œ ê¶Œì¥)")

    if not keyword:
        st.info("ì™¼ìª½ ìƒë‹¨ì— í‚¤ì›Œë“œë¥¼ ì…ë ¥í•œ ë’¤ Enter ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        return

    if st.button("ğŸ” í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰", type="primary"):
        try:
            with st.spinner("YouTube ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                df = fetch_videos_by_keyword(api_key, keyword, video_limit)
        except HttpError as e:
            msg = str(e)
            if "quotaExceeded" in msg:
                st.error("âŒ YouTube API ì¼ì¼ í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜, ê°€ì ¸ì˜¬ ì˜ìƒ ìˆ˜ë¥¼ ì¤„ì—¬ ì£¼ì„¸ìš”.")
            elif "keyInvalid" in msg:
                st.error("âŒ YouTube API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
            else:
                st.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {msg}")
            return

        if df.empty:
            st.warning("ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        render_basic_stats_cards_for_videos(df, f"'{keyword}' ê´€ë ¨ ì˜ìƒ ìš”ì•½")
        render_top_thumbnails(df)
        render_pattern_charts(df)
        render_keyword_suggestions(df)
        render_video_table(df)


def page_single_channel(api_key: str, video_limit: int):
    render_header()
    st.markdown("### ğŸ¯ íŠ¹ì • ì±„ë„ ì‹¬ì¸µ ë¶„ì„")

    raw_input = st.text_input(
        "ì±„ë„ ID ë˜ëŠ” ì±„ë„ URLì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: UC ë¡œ ì‹œì‘í•˜ëŠ” ID, ë˜ëŠ” https://www.youtube.com/channel/ í˜•íƒœ)",
        help="ë³µì¡í•œ URL(ì»¤ìŠ¤í…€ í•¸ë“¤ ë“±)ì€ ë‹¨ìˆœ ì±„ë„ ID ë¡œ ë³€í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ˆ ë  ê²½ìš°, ìœ íŠœë¸Œ ìŠ¤íŠœë””ì˜¤ì—ì„œ 'ì±„ë„ ID(UC...)' ë¥¼ ë³µì‚¬í•´ ì˜¤ì„¸ìš”.",
    )

    if not raw_input:
        st.info("ë¶„ì„í•  ì±„ë„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        return

    channel_id = extract_channel_id(raw_input)

    if st.button("ğŸ“Š ì±„ë„ ë¶„ì„ ì‹¤í–‰", type="primary"):
        try:
            with st.spinner("ì±„ë„/ì˜ìƒ ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                info = fetch_channel_basic(api_key, channel_id)
                df = fetch_channel_recent_videos(api_key, channel_id, video_limit)
        except HttpError as e:
            msg = str(e)
            if "quotaExceeded" in msg:
                st.error("âŒ YouTube API ì¼ì¼ í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜, ê°€ì ¸ì˜¬ ì˜ìƒ ìˆ˜ë¥¼ ì¤„ì—¬ ì£¼ì„¸ìš”.")
            elif "keyInvalid" in msg:
                st.error("âŒ YouTube API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
            else:
                st.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {msg}")
            return

        if not info:
            st.error("ì±„ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì±„ë„ ID/URLì„ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
            return

        # ì±„ë„ í—¤ë”
        c1, c2 = st.columns([1, 3])
        with c1:
            if info.get("thumbnail_url"):
                st.image(info["thumbnail_url"], caption=info["title"])
        with c2:
            st.markdown(f"## ğŸ“º {info['title']}")
            st.markdown(info.get("description", "")[:250] + "...")
            col_a, col_b, col_c = st.columns(3)
            col_a.metric("êµ¬ë…ì ìˆ˜", f"{info['subscriber_count']:,}")
            col_b.metric("ì´ ì¡°íšŒìˆ˜", f"{info['view_count']:,}")
            col_c.metric("ì´ ì—…ë¡œë“œ ìˆ˜", f"{info['video_count']:,}")

        render_basic_stats_cards_for_videos(df, "ìµœê·¼ ì—…ë¡œë“œ ì˜ìƒ ìš”ì•½")
        render_top_thumbnails(df)
        render_pattern_charts(df)

        st.subheader("ğŸ§  ì±„ë„ ìš´ì˜ ì¸ì‚¬ì´íŠ¸ (ë£° ê¸°ë°˜ ìš”ì•½)")
        st.write(make_simple_summary_for_channel(df))

        render_keyword_suggestions(df)
        render_video_table(df)


def page_competitive_channels(api_key: str, video_limit: int):
    render_header()
    st.markdown("### ğŸ¯ ê²½ìŸ ì±„ë„ ë²¤ì¹˜ë§ˆí‚¹")

    st.write(
        "ë¹„ìŠ·í•œ ì£¼ì œì˜ ì±„ë„ ì—¬ëŸ¬ ê°œë¥¼ ë„£ì–´ë‘ê³ , ìµœê·¼ ì˜ìƒ ì„±ê³¼ë¥¼ ê°„ë‹¨íˆ ë¹„êµí•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        "ê° ì¤„ì— í•˜ë‚˜ì”© ì±„ë„ ID ë˜ëŠ” URL ì„ ì ì–´ ì£¼ì„¸ìš”."
    )

    raw = st.text_area(
        "ì±„ë„ ID / URL ëª©ë¡ (í•œ ì¤„ì— í•˜ë‚˜ì”©)",
        height=150,
        placeholder="ì˜ˆ)\nhttps://www.youtube.com/channel/UCxxxxxxxxxx1\nhttps://www.youtube.com/channel/UCxxxxxxxxxx2",
    )

    if not raw.strip():
        st.info("ì±„ë„ ëª©ë¡ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        return

    lines = [extract_channel_id(line) for line in raw.splitlines() if line.strip()]
    lines = list(dict.fromkeys(lines))  # ì¤‘ë³µ ì œê±°

    if len(lines) < 2:
        st.info("ìµœì†Œ 2ê°œ ì´ìƒì˜ ì±„ë„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        return

    if st.button("ğŸ“Š ê²½ìŸ ì±„ë„ ë¹„êµ ì‹¤í–‰", type="primary"):
        rows = []
        error_channels = []

        for cid in lines:
            try:
                with st.spinner(f"ì±„ë„ {cid} ë¶„ì„ ì¤‘..."):
                    info = fetch_channel_basic(api_key, cid)
                    df = fetch_channel_recent_videos(api_key, cid, video_limit)
            except HttpError as e:
                msg = str(e)
                if "quotaExceeded" in msg:
                    st.error("âŒ YouTube API ì¼ì¼ í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ì´ìƒ ì±„ë„ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
                else:
                    error_channels.append(f"{cid} (ì˜¤ë¥˜: {msg})")
                    continue

            if not info or df.empty:
                error_channels.append(f"{cid} (ë°ì´í„° ì—†ìŒ)")
                continue

            row = {
                "ì±„ë„ëª…": info["title"],
                "ì±„ë„ID": info["channel_id"],
                "êµ¬ë…ì ìˆ˜": info["subscriber_count"],
                "ì´ ì¡°íšŒìˆ˜": info["view_count"],
                "ì´ ì—…ë¡œë“œ ìˆ˜": info["video_count"],
                "ìµœê·¼ ì˜ìƒ ìˆ˜(ë¶„ì„)": len(df),
                "ìµœê·¼ í‰ê·  ì¡°íšŒìˆ˜": int(df["views"].mean()),
                "ìµœê·¼ ì¤‘ì•™ê°’ ì¡°íšŒìˆ˜": int(df["views"].median()),
                "ìµœê·¼ ìµœê³  ì¡°íšŒìˆ˜": int(df["views"].max()),
                "ìµœê·¼ í‰ê·  ê¸¸ì´(ë¶„)": round(df["duration_min"].mean(), 1),
                "ìµœê·¼ ì¼ í‰ê·  ì¡°íšŒìˆ˜(í‰ê· )": int(df["views_per_day"].mean()),
            }
            rows.append(row)

        if not rows:
            st.error("ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ ì±„ë„ì´ ì—†ìŠµë‹ˆë‹¤.")
            if error_channels:
                st.write("ì˜¤ë¥˜ ì±„ë„ ëª©ë¡:")
                st.write("\n".join(error_channels))
            return

        result_df = pd.DataFrame(rows)
        st.subheader("ğŸ ê²½ìŸ ì±„ë„ ìš”ì•½ ë¹„êµ í…Œì´ë¸”")
        st.dataframe(result_df, use_container_width=True, hide_index=True)

        st.subheader("ğŸ“ˆ ì±„ë„ë³„ ìµœê·¼ í‰ê·  ì¡°íšŒìˆ˜ ë¹„êµ")
        st.bar_chart(result_df.set_index("ì±„ë„ëª…")["ìµœê·¼ í‰ê·  ì¡°íšŒìˆ˜"])

        st.subheader("ğŸ“ˆ ì±„ë„ë³„ ìµœê·¼ ì¼ í‰ê·  ì¡°íšŒìˆ˜(ì˜ìƒ 1ê°œ ê¸°ì¤€) ë¹„êµ")
        st.bar_chart(result_df.set_index("ì±„ë„ëª…")["ìµœê·¼ ì¼ í‰ê·  ì¡°íšŒìˆ˜(í‰ê· )"])

        if error_channels:
            with st.expander("âš  ë¶„ì„ ì‹¤íŒ¨/ë°ì´í„° ë¶€ì¡± ì±„ë„ ë³´ê¸°"):
                st.write("\n".join(error_channels))


# ----------------------------
# ë©”ì¸
# ----------------------------

def main():
    api_key = get_api_key()
    if not api_key:
        st.stop()

    # ì™¼ìª½ ì‚¬ì´ë“œë°” - ë¶„ì„ ëª¨ë“œ & ê³µí†µ ì˜µì…˜
    st.sidebar.header("ğŸ” ë¶„ì„ ëª¨ë“œ ì„ íƒ")

    mode = st.sidebar.radio(
        "ë¶„ì„ ëŒ€ìƒ",
        ["í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„", "íŠ¹ì • ì±„ë„ ë¶„ì„", "ê²½ìŸ ì±„ë„ ë²¤ì¹˜ë§ˆí‚¹"],
        index=0,
    )

    st.sidebar.markdown("---")
    video_limit = st.sidebar.slider(
        "ê°€ì ¸ì˜¬ ì˜ìƒ ê°œìˆ˜ (1íšŒ ë¶„ì„ë‹¹)",
        min_value=5,
        max_value=30,
        value=10,
        help="ê°’ì´ í´ìˆ˜ë¡ ë¶„ì„ì€ í’ë¶€í•´ì§€ì§€ë§Œ, YouTube API ì¼ì¼ í• ë‹¹ëŸ‰ì´ ë” ë¹¨ë¦¬ ì†Œëª¨ë©ë‹ˆë‹¤. 5~15 ê¶Œì¥.",
    )

    st.sidebar.markdown("---")
    st.sidebar.markdown(
        """
        **YouTube API ì¿¼í„° ì ˆì•½ íŒ**
        - ì˜ìƒ ê°œìˆ˜ëŠ” 5~15ê°œ ì •ë„ë¡œ ìœ ì§€  
        - ê°™ì€ í‚¤ì›Œë“œ/ì±„ë„ì„ ë°˜ë³µí•´ì„œ ìƒˆë¡œê³ ì¹¨í•˜ì§€ ì•Šê¸°  
        - ì˜¤ëŠ˜ ì¿¼í„°ê°€ ì´ˆê³¼ë˜ë©´ ë‚´ì¼ ìë™ìœ¼ë¡œ ì´ˆê¸°í™”ë¨
        """
    )

    if mode == "í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„":
        page_keyword_trend(api_key, video_limit)
    elif mode == "íŠ¹ì • ì±„ë„ ë¶„ì„":
        page_single_channel(api_key, video_limit)
    elif mode == "ê²½ìŸ ì±„ë„ ë²¤ì¹˜ë§ˆí‚¹":
        page_competitive_channels(api_key, video_limit)


if __name__ == "__main__":
    main()
